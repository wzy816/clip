{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbdb3fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import *\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CLIPTextConfig:\n",
    "    vocab_size: int = 49408\n",
    "    hidden_size: int = 512\n",
    "    intermediate_size: int = 2048\n",
    "    projection_dim: int = 512\n",
    "    num_hidden_layers: int = 12\n",
    "    num_attention_heads: int = 8\n",
    "    max_position_embeddings: int = 77\n",
    "    layer_norm_eps: float = 1e-5\n",
    "    attention_dropout: float = 0.0\n",
    "    pad_token_id: int = 49407\n",
    "    bos_token_id: int = 49406\n",
    "    eos_token_id: int = 49407\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CLIPVisionConfig:\n",
    "    hidden_size: int = 768\n",
    "    intermediate_size: int = 3072\n",
    "    projection_dim: int = 512\n",
    "    num_hidden_layers: int = 12\n",
    "    num_attention_heads: int = 12\n",
    "    num_channels: int = 3\n",
    "    image_size: int = 224\n",
    "    patch_size: int = 32\n",
    "    layer_norm_eps: float = 1e-5\n",
    "    attention_dropout: float = 0.0\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CLIPConfig:\n",
    "    text_config: CLIPTextConfig = CLIPTextConfig()\n",
    "    vision_config: CLIPVisionConfig = CLIPVisionConfig()\n",
    "    projection_dim: int = 512\n",
    "    logit_scale_init_value: float = 2.6592\n",
    "\n",
    "\n",
    "class QuickGELUActivation(nn.Module):\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        return input * torch.sigmoid(1.702 * input)\n",
    "\n",
    "\n",
    "class CLIPMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.activation_fn = QuickGELUActivation()\n",
    "        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.fc1(hidden_states)\n",
    "        hidden_states = self.activation_fn(hidden_states)\n",
    "        hidden_states = self.fc2(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class CLIPVisionEmbeddings(nn.Module):\n",
    "    def __init__(self, config: CLIPVisionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.image_size = config.image_size\n",
    "        self.patch_size = config.patch_size\n",
    "\n",
    "        self.class_embedding = nn.Parameter(torch.randn(self.embed_dim))\n",
    "\n",
    "        self.patch_embedding = nn.Conv2d(\n",
    "            in_channels=config.num_channels,\n",
    "            out_channels=self.embed_dim,\n",
    "            kernel_size=self.patch_size,\n",
    "            stride=self.patch_size,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        self.num_patches = (self.image_size // self.patch_size) ** 2\n",
    "        self.num_positions = self.num_patches + 1\n",
    "        self.position_embedding = nn.Embedding(self.num_positions, self.embed_dim)\n",
    "        self.register_buffer(\n",
    "            \"position_ids\",\n",
    "            torch.arange(self.num_positions).expand((1, -1)),\n",
    "            persistent=False,\n",
    "        )\n",
    "\n",
    "    def forward(self, pixel_values: torch.FloatTensor) -> torch.Tensor:\n",
    "        batch_size = pixel_values.shape[0]\n",
    "        target_dtype = self.patch_embedding.weight.dtype\n",
    "        patch_embeds = self.patch_embedding(\n",
    "            pixel_values.to(dtype=target_dtype)\n",
    "        )  # shape = [*, width, grid, grid]\n",
    "        patch_embeds = patch_embeds.flatten(2).transpose(1, 2)\n",
    "\n",
    "        class_embeds = self.class_embedding.expand(batch_size, 1, -1)\n",
    "        embeddings = torch.cat([class_embeds, patch_embeds], dim=1)\n",
    "        embeddings = embeddings + self.position_embedding(self.position_ids)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class CLIPTextEmbeddings(nn.Module):\n",
    "    def __init__(self, config: CLIPTextConfig):\n",
    "        super().__init__()\n",
    "        embed_dim = config.hidden_size\n",
    "\n",
    "        self.token_embedding = nn.Embedding(config.vocab_size, embed_dim)\n",
    "        self.position_embedding = nn.Embedding(\n",
    "            config.max_position_embeddings, embed_dim\n",
    "        )\n",
    "\n",
    "        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n",
    "        self.register_buffer(\n",
    "            \"position_ids\",\n",
    "            torch.arange(config.max_position_embeddings).expand((1, -1)),\n",
    "            persistent=False,\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "    ) -> torch.Tensor:\n",
    "        seq_length = (\n",
    "            input_ids.shape[-1] if input_ids is not None else inputs_embeds.shape[-2]\n",
    "        )\n",
    "\n",
    "        position_ids = self.position_ids[:, :seq_length]\n",
    "        inputs_embeds = self.token_embedding(input_ids)\n",
    "\n",
    "        position_embeddings = self.position_embedding(position_ids)\n",
    "        embeddings = inputs_embeds + position_embeddings\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class CLIPAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "        self.scale = self.head_dim**-0.5\n",
    "        self.dropout = config.attention_dropout\n",
    "\n",
    "        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "\n",
    "    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n",
    "        return (\n",
    "            tensor.view(bsz, seq_len, self.num_heads, self.head_dim)\n",
    "            .transpose(1, 2)\n",
    "            .contiguous()\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        causal_attention_mask: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n",
    "\n",
    "        bsz, tgt_len, embed_dim = hidden_states.size()\n",
    "\n",
    "        # get query proj\n",
    "        query_states = self.q_proj(hidden_states) * self.scale\n",
    "        key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
    "        value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
    "\n",
    "        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n",
    "        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
    "        key_states = key_states.view(*proj_shape)\n",
    "        value_states = value_states.view(*proj_shape)\n",
    "\n",
    "        src_len = key_states.size(1)\n",
    "        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
    "\n",
    "        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
    "            raise ValueError(\n",
    "                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n",
    "                f\" {attn_weights.size()}\"\n",
    "            )\n",
    "\n",
    "        if causal_attention_mask is not None:\n",
    "            if causal_attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
    "                raise ValueError(\n",
    "                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is\"\n",
    "                    f\" {causal_attention_mask.size()}\"\n",
    "                )\n",
    "            attn_weights = (\n",
    "                attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
    "                + causal_attention_mask\n",
    "            )\n",
    "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
    "\n",
    "        attn_probs = nn.functional.dropout(\n",
    "            attn_weights, p=self.dropout, training=self.training\n",
    "        )\n",
    "        attn_output = torch.bmm(attn_probs, value_states)\n",
    "\n",
    "        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
    "            raise ValueError(\n",
    "                f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is\"\n",
    "                f\" {attn_output.size()}\"\n",
    "            )\n",
    "\n",
    "        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
    "        attn_output = attn_output.transpose(1, 2)\n",
    "        attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)\n",
    "\n",
    "        return self.out_proj(attn_output)\n",
    "\n",
    "\n",
    "class CLIPEncoderLayer(nn.Module):\n",
    "    def __init__(self, config: Union[CLIPTextConfig, CLIPVisionConfig]):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.self_attn = CLIPAttention(config)\n",
    "        self.layer_norm1 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n",
    "        self.mlp = CLIPMLP(config)\n",
    "        self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        causal_attention_mask: torch.Tensor,\n",
    "    ) -> torch.FloatTensor:\n",
    "        residual = hidden_states\n",
    "\n",
    "        hidden_states = self.layer_norm1(hidden_states)\n",
    "        hidden_states = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            causal_attention_mask=causal_attention_mask,\n",
    "        )\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.layer_norm2(hidden_states)\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class CLIPEncoder(nn.Module):\n",
    "    def __init__(self, config: Union[CLIPTextConfig, CLIPVisionConfig]):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layers = nn.ModuleList(\n",
    "            [CLIPEncoderLayer(config) for _ in range(config.num_hidden_layers)]\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        inputs_embeds,\n",
    "        causal_attention_mask: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        hidden_states = inputs_embeds\n",
    "        for idx, encoder_layer in enumerate(self.layers):\n",
    "            hidden_states = encoder_layer(\n",
    "                hidden_states,\n",
    "                causal_attention_mask,\n",
    "            )\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class CLIPTextModel(nn.Module):\n",
    "    def __init__(self, config: CLIPTextConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        embed_dim = config.hidden_size\n",
    "        self.embeddings = CLIPTextEmbeddings(config)\n",
    "        self.encoder = CLIPEncoder(config)\n",
    "        self.final_layer_norm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n",
    "\n",
    "        self.eos_token_id = config.eos_token_id\n",
    "\n",
    "    def _create_4d_causal_attention_mask(self, input_shape, dtype):\n",
    "        bsz = input_shape[0]\n",
    "        l = input_shape[-1]\n",
    "        mask = torch.full((l, l), torch.finfo(dtype).min)\n",
    "        mask_cond = torch.arange(mask.size(-1))\n",
    "        mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)\n",
    "        mask = mask.to(dtype)\n",
    "        mask = mask[None, None, :, :].expand(bsz, 1, l, l)\n",
    "        return mask\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        if input_ids is None:\n",
    "            raise ValueError(\"You have to specify input_ids\")\n",
    "\n",
    "        input_shape = input_ids.size()\n",
    "        input_ids = input_ids.view(-1, input_shape[-1])\n",
    "\n",
    "        hidden_states = self.embeddings(input_ids=input_ids)\n",
    "\n",
    "        causal_attention_mask = self._create_4d_causal_attention_mask(\n",
    "            input_shape, hidden_states.dtype\n",
    "        )\n",
    "\n",
    "        last_hidden_state = self.encoder(\n",
    "            inputs_embeds=hidden_states,\n",
    "            causal_attention_mask=causal_attention_mask,\n",
    "        )\n",
    "\n",
    "        last_hidden_state = self.final_layer_norm(last_hidden_state)\n",
    "\n",
    "        pooled_output = last_hidden_state[\n",
    "            torch.arange(last_hidden_state.shape[0], device=last_hidden_state.device),\n",
    "            (\n",
    "                input_ids.to(dtype=torch.int, device=last_hidden_state.device)\n",
    "                == self.eos_token_id\n",
    "            )\n",
    "            .int()\n",
    "            .argmax(dim=-1),\n",
    "        ]\n",
    "        return last_hidden_state, pooled_output\n",
    "\n",
    "\n",
    "class CLIPVisionModel(nn.Module):\n",
    "    def __init__(self, config: CLIPVisionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.embeddings = CLIPVisionEmbeddings(config)\n",
    "        self.pre_layrnorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.encoder = CLIPEncoder(config)\n",
    "        self.post_layernorm = nn.LayerNorm(\n",
    "            config.hidden_size, eps=config.layer_norm_eps\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: torch.FloatTensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        hidden_states = self.embeddings(pixel_values)\n",
    "        hidden_states = self.pre_layrnorm(hidden_states)\n",
    "\n",
    "        last_hidden_state = self.encoder(\n",
    "            inputs_embeds=hidden_states,\n",
    "        )\n",
    "\n",
    "        pooled_output = last_hidden_state[:, 0, :]\n",
    "        pooled_output = self.post_layernorm(pooled_output)\n",
    "\n",
    "        return last_hidden_state, pooled_output\n",
    "\n",
    "\n",
    "class CLIPModel(nn.Module):\n",
    "    def __init__(self, config: CLIPConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.projection_dim = config.projection_dim\n",
    "        self.text_embed_dim = config.text_config.hidden_size\n",
    "        self.vision_embed_dim = config.vision_config.hidden_size\n",
    "\n",
    "        self.text_model = CLIPTextModel(config.text_config)\n",
    "        self.vision_model = CLIPVisionModel(config.vision_config)\n",
    "\n",
    "        self.visual_projection = nn.Linear(\n",
    "            self.vision_embed_dim, self.projection_dim, bias=False\n",
    "        )\n",
    "        self.text_projection = nn.Linear(\n",
    "            self.text_embed_dim, self.projection_dim, bias=False\n",
    "        )\n",
    "        self.logit_scale = nn.Parameter(\n",
    "            torch.tensor(self.config.logit_scale_init_value)\n",
    "        )\n",
    "\n",
    "    def get_text_features(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        last_hidden_state, pooled_output = self.text_model(\n",
    "            input_ids=input_ids,\n",
    "        )\n",
    "\n",
    "        return last_hidden_state, self.text_projection(pooled_output)\n",
    "\n",
    "    def get_image_features(\n",
    "        self,\n",
    "        pixel_values: torch.FloatTensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        last_hidden_state, pooled_output = self.vision_model(\n",
    "            pixel_values=pixel_values,\n",
    "        )\n",
    "\n",
    "        return last_hidden_state, self.visual_projection(pooled_output)\n",
    "\n",
    "    def clip_loss(self, similarity: torch.Tensor) -> torch.Tensor:\n",
    "        caption_loss = nn.functional.cross_entropy(\n",
    "            similarity, torch.arange(len(similarity))\n",
    "        )\n",
    "        image_loss = nn.functional.cross_entropy(\n",
    "            similarity.t(), torch.arange(len(similarity.t()))\n",
    "        )\n",
    "        return (caption_loss + image_loss) / 2.0\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        pixel_values: torch.FloatTensor,\n",
    "    ) -> Tuple[\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "    ]:\n",
    "        image_last_hidden_states, image_embeds = self.get_image_features(\n",
    "            pixel_values=pixel_values,\n",
    "        )\n",
    "\n",
    "        text_last_hidden_states, text_embeds = self.get_text_features(\n",
    "            input_ids=input_ids,\n",
    "        )\n",
    "\n",
    "        image_embeds = image_embeds / image_embeds.norm(dim=1, keepdim=True)\n",
    "        text_embeds = text_embeds / text_embeds.norm(dim=1, keepdim=True)\n",
    "\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits_per_image = logit_scale * image_embeds @ text_embeds.t()\n",
    "        logits_per_text = logits_per_image.t()\n",
    "\n",
    "        return (\n",
    "            logits_per_image,\n",
    "            logits_per_text,\n",
    "            text_embeds,\n",
    "            image_embeds,\n",
    "            image_last_hidden_states,\n",
    "            text_last_hidden_states,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8644e3df",
   "metadata": {},
   "source": [
    "## prepare model file\n",
    "- download https://huggingface.co/openai/clip-vit-base-patch32/blob/main/pytorch_model.bin to /mnt/clip/clip-vit-base-patch32/pytorch_model.bin\n",
    "- download https://huggingface.co/openai/clip-vit-base-patch32/blob/main/merges.txt to /mnt/clip/clip-vit-base-patch32/merges.txt\n",
    "- download https://huggingface.co/openai/clip-vit-base-patch32/blob/main/vocab.json to /mnt/clip/clip-vit-base-patch32/vocab.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b4e0806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIPModel(\n",
      "  (text_model): CLIPTextModel(\n",
      "    (embeddings): CLIPTextEmbeddings(\n",
      "      (token_embedding): Embedding(49408, 512)\n",
      "      (position_embedding): Embedding(77, 512)\n",
      "    )\n",
      "    (encoder): CLIPEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (vision_model): CLIPVisionModel(\n",
      "    (embeddings): CLIPVisionEmbeddings(\n",
      "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
      "      (position_embedding): Embedding(50, 768)\n",
      "    )\n",
      "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (encoder): CLIPEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
      "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "torch.set_default_device(\"cuda\")\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "checkpoint_path = '/mnt/clip/clip-vit-base-patch32/pytorch_model.bin'\n",
    "state_dict = torch.load(checkpoint_path, map_location=\"cuda:0\")\n",
    "\n",
    "model_config = CLIPConfig()\n",
    "clip_model = CLIPModel(model_config)\n",
    "clip_model.load_state_dict(state_dict, strict=False)\n",
    "print(clip_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ab9e9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from functools import lru_cache\n",
    "from typing import *\n",
    "\n",
    "import regex as re\n",
    "\n",
    "import ftfy\n",
    "\n",
    "\n",
    "@lru_cache()\n",
    "def bytes_to_unicode():\n",
    "    \"\"\"\n",
    "    Returns list of utf-8 byte and a mapping to unicode strings. We specifically avoids mapping to whitespace/control\n",
    "    characters the bpe code barfs on.\n",
    "\n",
    "    The reversible bpe codes work on unicode strings. This means you need a large # of unicode characters in your vocab\n",
    "    if you want to avoid UNKs. When you're at something like a 10B token dataset you end up needing around 5K for\n",
    "    decent coverage. This is a significant percentage of your normal, say, 32K bpe vocab. To avoid that, we want lookup\n",
    "    tables between utf-8 bytes and unicode strings.\n",
    "    \"\"\"\n",
    "    bs = (\n",
    "        list(range(ord(\"!\"), ord(\"~\") + 1))\n",
    "        + list(range(ord(\"¡\"), ord(\"¬\") + 1))\n",
    "        + list(range(ord(\"®\"), ord(\"ÿ\") + 1))\n",
    "    )\n",
    "    cs = bs[:]\n",
    "    n = 0\n",
    "    for b in range(2**8):\n",
    "        if b not in bs:\n",
    "            bs.append(b)\n",
    "            cs.append(2**8 + n)\n",
    "            n += 1\n",
    "    cs = [chr(n) for n in cs]\n",
    "    return dict(zip(bs, cs))\n",
    "\n",
    "\n",
    "def get_pairs(word):\n",
    "    \"\"\"\n",
    "    Return set of symbol pairs in a word.\n",
    "\n",
    "    Word is represented as tuple of symbols (symbols being variable-length strings).\n",
    "    \"\"\"\n",
    "    pairs = set()\n",
    "    prev_char = word[0]\n",
    "    for char in word[1:]:\n",
    "        pairs.add((prev_char, char))\n",
    "        prev_char = char\n",
    "    return pairs\n",
    "\n",
    "\n",
    "class CLIPTokenizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_file,\n",
    "        merges_file,\n",
    "    ):\n",
    "        with open(vocab_file, encoding=\"utf-8\") as vocab_handle:\n",
    "            self.encoder = json.load(vocab_handle)\n",
    "\n",
    "        self.decoder = {v: k for k, v in self.encoder.items()}\n",
    "\n",
    "        self.byte_encoder = bytes_to_unicode()\n",
    "        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n",
    "\n",
    "        with open(merges_file, encoding=\"utf-8\") as merges_handle:\n",
    "            bpe_merges = (\n",
    "                merges_handle.read().strip().split(\"\\n\")[1 : 49152 - 256 - 2 + 1]\n",
    "            )  # =[1:]\n",
    "        bpe_merges = [tuple(merge.split()) for merge in bpe_merges]\n",
    "        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n",
    "\n",
    "        self.cache = {\n",
    "            \"<|startoftext|>\": \"<|startoftext|>\",\n",
    "            \"<|endoftext|>\": \"<|endoftext|>\",\n",
    "        }\n",
    "\n",
    "        self.pat = re.compile(\n",
    "            r\"\"\"<\\|startoftext\\|>|<\\|endoftext\\|>|'s|'t|'re|'ve|'m|'ll|'d|[\\p{L}]+|[\\p{N}]|[^\\s\\p{L}\\p{N}]+\"\"\",\n",
    "            re.IGNORECASE,\n",
    "        )\n",
    "\n",
    "        self.unk_token = \"<|endoftext|>\"\n",
    "        self.bos_token = \"<|startoftext|>\"\n",
    "        self.eos_token = \"<|endoftext|>\"\n",
    "        self.pad_token = \"<|endoftext|>\"\n",
    "\n",
    "        self.unk_token_id = 49407\n",
    "        self.bos_token_id = 49406\n",
    "        self.eos_token_id = 49407\n",
    "        self.pad_token_id = 49407\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.encoder)\n",
    "\n",
    "    def bpe(self, token):\n",
    "        if token in self.cache:\n",
    "            return self.cache[token]\n",
    "\n",
    "        word = tuple(token[:-1]) + (token[-1] + \"</w>\",)\n",
    "        pairs = get_pairs(word)\n",
    "\n",
    "        if not pairs:\n",
    "            return token + \"</w>\"\n",
    "\n",
    "        while True:\n",
    "            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float(\"inf\")))\n",
    "            if bigram not in self.bpe_ranks:\n",
    "                break\n",
    "            first, second = bigram\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                try:\n",
    "                    j = word.index(first, i)\n",
    "                except ValueError:\n",
    "                    new_word.extend(word[i:])\n",
    "                    break\n",
    "                else:\n",
    "                    new_word.extend(word[i:j])\n",
    "                    i = j\n",
    "\n",
    "                if word[i] == first and i < len(word) - 1 and word[i + 1] == second:\n",
    "                    new_word.append(first + second)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            new_word = tuple(new_word)\n",
    "            word = new_word\n",
    "            if len(word) == 1:\n",
    "                break\n",
    "            else:\n",
    "                pairs = get_pairs(word)\n",
    "        word = \" \".join(word)\n",
    "        self.cache[token] = word\n",
    "        return word\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = self._tokenize(text)\n",
    "        return [self.encoder.get(t, self.unk_token_id) for t in tokens]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        tokens = [self.decoder.get(index) for index in ids]\n",
    "        return self.convert_tokens_to_string(tokens)\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        bpe_tokens = []\n",
    "\n",
    "        text = ftfy.fix_text(text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "        text = text.strip().lower()\n",
    "\n",
    "        for token in re.findall(self.pat, text):\n",
    "            token = \"\".join(\n",
    "                self.byte_encoder[b] for b in token.encode(\"utf-8\")\n",
    "            )  # Maps all our bytes to unicode strings, avoiding control tokens of the BPE (spaces in our case)\n",
    "            bpe_tokens.extend(bpe_token for bpe_token in self.bpe(token).split(\" \"))\n",
    "        return bpe_tokens\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        text = \"\".join(tokens)\n",
    "        byte_array = bytearray([self.byte_decoder[c] for c in text])\n",
    "        text = byte_array.decode(\"utf-8\", errors=\"replace\").replace(\"</w>\", \" \").strip()\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bdbddec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[49406, 320, 1125, 320, 2368, 49407]\n",
      "torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "vocab_file = '/mnt/clip/clip-vit-base-patch32/vocab.json'\n",
    "merges_file = '/mnt/clip/clip-vit-base-patch32/merges.txt'\n",
    "tokenizer = CLIPTokenizer(vocab_file, merges_file)\n",
    "\n",
    "input_ids = tokenizer.encode('a photo a cat')\n",
    "input_ids = [tokenizer.bos_token_id] + input_ids + [tokenizer.eos_token_id]\n",
    "print(input_ids)\n",
    "\n",
    "assert len(input_ids) < model_config.text_config.max_position_embeddings\n",
    "\n",
    "input_ids = torch.LongTensor([input_ids]).cuda()\n",
    "\n",
    "_, text_embeddings = clip_model.get_text_features(input_ids)\n",
    "text_embeddings = text_embeddings / text_embeddings.norm(dim=1,keepdim=True)\n",
    "\n",
    "text_embeddings = text_embeddings[0]\n",
    "print(text_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1c5fa93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 512])\n",
      "torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "image_mean = [0.48145466,0.4578275,0.40821073]\n",
    "image_std = [0.26862954,0.26130258,0.27577711]\n",
    "\n",
    "image_paths = ['./data/n02687172_aircraft_carrier.jpeg', './data/n02974003_car_wheel.jpeg']\n",
    "images = []\n",
    "for image_path in image_paths:\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    resized_image = image.resize((224,224), resample=Image.Resampling.BICUBIC)\n",
    "    image_arr = np.array(resized_image) # (height,width,3)\n",
    "    image_arr = image_arr * 1/225.0\n",
    "    normalized_image_arr = (image_arr - image_mean) / image_std\n",
    "    normalized_image_arr = normalized_image_arr.transpose(2, 0, 1) # (c, h, w)\n",
    "    images.append(normalized_image_arr)\n",
    "\n",
    "images = np.array(images)\n",
    "    \n",
    "    \n",
    "# inference\n",
    "pixel_values = torch.Tensor(images).cuda()\n",
    "_, image_embeddings = clip_model.get_image_features(pixel_values)    \n",
    "image_embeddings = image_embeddings / image_embeddings.norm(dim=1,keepdim=True)\n",
    "print(image_embeddings.shape)\n",
    "\n",
    "# mean\n",
    "image_embeddings_mean = torch.mean(image_embeddings,dim=0)\n",
    "\n",
    "print(image_embeddings_mean.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd2932a1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7f94b1f392d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "sns.displot(np.array(image_embeddings_mean.tolist()))\n",
    "sns.displot(np.array(text_embeddings.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9535619",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
