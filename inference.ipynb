{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbdb3fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import *\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CLIPTextConfig:\n",
    "    vocab_size: int = 49408\n",
    "    hidden_size: int = 512\n",
    "    intermediate_size: int = 2048\n",
    "    projection_dim: int = 512\n",
    "    num_hidden_layers: int = 12\n",
    "    num_attention_heads: int = 8\n",
    "    max_position_embeddings: int = 77\n",
    "    layer_norm_eps: float = 1e-5\n",
    "    attention_dropout: float = 0.0\n",
    "    # initializer_range: float = 0.02\n",
    "    # initializer_factor: float = 1.0\n",
    "    pad_token_id: int = 1\n",
    "    bos_token_id: int = 49406  # 0\n",
    "    eos_token_id: int = 49407  # 2\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CLIPVisionConfig:\n",
    "    hidden_size: int = 768\n",
    "    intermediate_size: int = 3072\n",
    "    projection_dim: int = 512\n",
    "    num_hidden_layers: int = 12\n",
    "    num_attention_heads: int = 12\n",
    "    num_channels: int = 3\n",
    "    image_size: int = 224\n",
    "    patch_size: int = 32\n",
    "    layer_norm_eps: float = 1e-5\n",
    "    attention_dropout: float = 0.0\n",
    "    # initializer_range: float = 0.02\n",
    "    # initializer_factor: float = 1.0\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CLIPConfig:\n",
    "    text_config: CLIPTextConfig = CLIPTextConfig()\n",
    "    vision_config: CLIPVisionConfig = CLIPVisionConfig()\n",
    "    projection_dim: int = 512\n",
    "    logit_scale_init_value: float = 2.6592\n",
    "    # initializer_factor: float = 1.0\n",
    "\n",
    "\n",
    "class QuickGELUActivation(nn.Module):\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        return input * torch.sigmoid(1.702 * input)\n",
    "\n",
    "\n",
    "class CLIPMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.activation_fn = QuickGELUActivation()\n",
    "        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.fc1(hidden_states)\n",
    "        hidden_states = self.activation_fn(hidden_states)\n",
    "        hidden_states = self.fc2(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class CLIPVisionEmbeddings(nn.Module):\n",
    "    def __init__(self, config: CLIPVisionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.image_size = config.image_size\n",
    "        self.patch_size = config.patch_size\n",
    "\n",
    "        self.class_embedding = nn.Parameter(torch.randn(self.embed_dim))\n",
    "\n",
    "        self.patch_embedding = nn.Conv2d(\n",
    "            in_channels=config.num_channels,\n",
    "            out_channels=self.embed_dim,\n",
    "            kernel_size=self.patch_size,\n",
    "            stride=self.patch_size,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        self.num_patches = (self.image_size // self.patch_size) ** 2\n",
    "        self.num_positions = self.num_patches + 1\n",
    "        self.position_embedding = nn.Embedding(self.num_positions, self.embed_dim)\n",
    "        self.register_buffer(\n",
    "            \"position_ids\",\n",
    "            torch.arange(self.num_positions).expand((1, -1)),\n",
    "            persistent=False,\n",
    "        )\n",
    "\n",
    "    def forward(self, pixel_values: torch.FloatTensor) -> torch.Tensor:\n",
    "        batch_size = pixel_values.shape[0]\n",
    "        target_dtype = self.patch_embedding.weight.dtype\n",
    "        patch_embeds = self.patch_embedding(\n",
    "            pixel_values.to(dtype=target_dtype)\n",
    "        )  # shape = [*, width, grid, grid]\n",
    "        patch_embeds = patch_embeds.flatten(2).transpose(1, 2)\n",
    "\n",
    "        class_embeds = self.class_embedding.expand(batch_size, 1, -1)\n",
    "        embeddings = torch.cat([class_embeds, patch_embeds], dim=1)\n",
    "        embeddings = embeddings + self.position_embedding(self.position_ids)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class CLIPTextEmbeddings(nn.Module):\n",
    "    def __init__(self, config: CLIPTextConfig):\n",
    "        super().__init__()\n",
    "        embed_dim = config.hidden_size\n",
    "\n",
    "        self.token_embedding = nn.Embedding(config.vocab_size, embed_dim)\n",
    "        self.position_embedding = nn.Embedding(\n",
    "            config.max_position_embeddings, embed_dim\n",
    "        )\n",
    "\n",
    "        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n",
    "        self.register_buffer(\n",
    "            \"position_ids\",\n",
    "            torch.arange(config.max_position_embeddings).expand((1, -1)),\n",
    "            persistent=False,\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "    ) -> torch.Tensor:\n",
    "        seq_length = (\n",
    "            input_ids.shape[-1] if input_ids is not None else inputs_embeds.shape[-2]\n",
    "        )\n",
    "\n",
    "        position_ids = self.position_ids[:, :seq_length]\n",
    "        inputs_embeds = self.token_embedding(input_ids)\n",
    "\n",
    "        position_embeddings = self.position_embedding(position_ids)\n",
    "        embeddings = inputs_embeds + position_embeddings\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class CLIPAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "        self.scale = self.head_dim**-0.5\n",
    "        self.dropout = config.attention_dropout\n",
    "\n",
    "        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "\n",
    "    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n",
    "        return (\n",
    "            tensor.view(bsz, seq_len, self.num_heads, self.head_dim)\n",
    "            .transpose(1, 2)\n",
    "            .contiguous()\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        causal_attention_mask: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n",
    "\n",
    "        bsz, tgt_len, embed_dim = hidden_states.size()\n",
    "\n",
    "        # get query proj\n",
    "        query_states = self.q_proj(hidden_states) * self.scale\n",
    "        key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
    "        value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
    "\n",
    "        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n",
    "        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
    "        key_states = key_states.view(*proj_shape)\n",
    "        value_states = value_states.view(*proj_shape)\n",
    "\n",
    "        src_len = key_states.size(1)\n",
    "        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
    "\n",
    "        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
    "            raise ValueError(\n",
    "                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n",
    "                f\" {attn_weights.size()}\"\n",
    "            )\n",
    "\n",
    "        if causal_attention_mask is not None:\n",
    "            if causal_attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
    "                raise ValueError(\n",
    "                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is\"\n",
    "                    f\" {causal_attention_mask.size()}\"\n",
    "                )\n",
    "            attn_weights = (\n",
    "                attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
    "                + causal_attention_mask\n",
    "            )\n",
    "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
    "\n",
    "        attn_probs = nn.functional.dropout(\n",
    "            attn_weights, p=self.dropout, training=self.training\n",
    "        )\n",
    "        attn_output = torch.bmm(attn_probs, value_states)\n",
    "\n",
    "        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
    "            raise ValueError(\n",
    "                f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is\"\n",
    "                f\" {attn_output.size()}\"\n",
    "            )\n",
    "\n",
    "        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
    "        attn_output = attn_output.transpose(1, 2)\n",
    "        attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)\n",
    "\n",
    "        return self.out_proj(attn_output)\n",
    "\n",
    "\n",
    "class CLIPEncoderLayer(nn.Module):\n",
    "    def __init__(self, config: Union[CLIPTextConfig, CLIPVisionConfig]):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.self_attn = CLIPAttention(config)\n",
    "        self.layer_norm1 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n",
    "        self.mlp = CLIPMLP(config)\n",
    "        self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        causal_attention_mask: torch.Tensor,\n",
    "    ) -> torch.FloatTensor:\n",
    "        residual = hidden_states\n",
    "\n",
    "        hidden_states = self.layer_norm1(hidden_states)\n",
    "        hidden_states = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            causal_attention_mask=causal_attention_mask,\n",
    "        )\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.layer_norm2(hidden_states)\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class CLIPEncoder(nn.Module):\n",
    "    def __init__(self, config: Union[CLIPTextConfig, CLIPVisionConfig]):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layers = nn.ModuleList(\n",
    "            [CLIPEncoderLayer(config) for _ in range(config.num_hidden_layers)]\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        inputs_embeds,\n",
    "        causal_attention_mask: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        hidden_states = inputs_embeds\n",
    "        for idx, encoder_layer in enumerate(self.layers):\n",
    "            hidden_states = encoder_layer(\n",
    "                hidden_states,\n",
    "                causal_attention_mask,\n",
    "            )\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class CLIPTextModel(nn.Module):\n",
    "    def __init__(self, config: CLIPTextConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        embed_dim = config.hidden_size\n",
    "        self.embeddings = CLIPTextEmbeddings(config)\n",
    "        self.encoder = CLIPEncoder(config)\n",
    "        self.final_layer_norm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n",
    "\n",
    "        self.eos_token_id = config.eos_token_id\n",
    "\n",
    "    def _create_4d_causal_attention_mask(self, input_shape, dtype):\n",
    "        bsz = input_shape[0]\n",
    "        l = input_shape[-1]\n",
    "        mask = torch.full((l, l), torch.finfo(dtype).min)\n",
    "        mask_cond = torch.arange(mask.size(-1))\n",
    "        mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)\n",
    "        mask = mask.to(dtype)\n",
    "        mask = mask[None, None, :, :].expand(bsz, 1, l, l)\n",
    "        return mask\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        if input_ids is None:\n",
    "            raise ValueError(\"You have to specify input_ids\")\n",
    "\n",
    "        input_shape = input_ids.size()\n",
    "        input_ids = input_ids.view(-1, input_shape[-1])\n",
    "\n",
    "        hidden_states = self.embeddings(input_ids=input_ids)\n",
    "\n",
    "        causal_attention_mask = self._create_4d_causal_attention_mask(\n",
    "            input_shape, hidden_states.dtype\n",
    "        )\n",
    "\n",
    "        last_hidden_state = self.encoder(\n",
    "            inputs_embeds=hidden_states,\n",
    "            causal_attention_mask=causal_attention_mask,\n",
    "        )\n",
    "\n",
    "        last_hidden_state = self.final_layer_norm(last_hidden_state)\n",
    "\n",
    "        pooled_output = last_hidden_state[\n",
    "            torch.arange(last_hidden_state.shape[0], device=last_hidden_state.device),\n",
    "            (\n",
    "                input_ids.to(dtype=torch.int, device=last_hidden_state.device)\n",
    "                == self.eos_token_id\n",
    "            )\n",
    "            .int()\n",
    "            .argmax(dim=-1),\n",
    "        ]\n",
    "        return last_hidden_state, pooled_output\n",
    "\n",
    "\n",
    "class CLIPVisionModel(nn.Module):\n",
    "    def __init__(self, config: CLIPVisionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.embeddings = CLIPVisionEmbeddings(config)\n",
    "        self.pre_layrnorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.encoder = CLIPEncoder(config)\n",
    "        self.post_layernorm = nn.LayerNorm(\n",
    "            config.hidden_size, eps=config.layer_norm_eps\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: torch.FloatTensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        hidden_states = self.embeddings(pixel_values)\n",
    "        hidden_states = self.pre_layrnorm(hidden_states)\n",
    "\n",
    "        last_hidden_state = self.encoder(\n",
    "            inputs_embeds=hidden_states,\n",
    "        )\n",
    "\n",
    "        pooled_output = last_hidden_state[:, 0, :]\n",
    "        pooled_output = self.post_layernorm(pooled_output)\n",
    "\n",
    "        return last_hidden_state, pooled_output\n",
    "\n",
    "\n",
    "class CLIPModel(nn.Module):\n",
    "    def __init__(self, config: CLIPConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.projection_dim = config.projection_dim\n",
    "        self.text_embed_dim = config.text_config.hidden_size\n",
    "        self.vision_embed_dim = config.vision_config.hidden_size\n",
    "\n",
    "        self.text_model = CLIPTextModel(config.text_config)\n",
    "        self.vision_model = CLIPVisionModel(config.vision_config)\n",
    "\n",
    "        self.visual_projection = nn.Linear(\n",
    "            self.vision_embed_dim, self.projection_dim, bias=False\n",
    "        )\n",
    "        self.text_projection = nn.Linear(\n",
    "            self.text_embed_dim, self.projection_dim, bias=False\n",
    "        )\n",
    "        self.logit_scale = nn.Parameter(\n",
    "            torch.tensor(self.config.logit_scale_init_value)\n",
    "        )\n",
    "\n",
    "    def get_text_features(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        last_hidden_state, pooled_output = self.text_model(\n",
    "            input_ids=input_ids,\n",
    "        )\n",
    "\n",
    "        return last_hidden_state, self.text_projection(pooled_output)\n",
    "\n",
    "    def get_image_features(\n",
    "        self,\n",
    "        pixel_values: torch.FloatTensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        last_hidden_state, pooled_output = self.vision_model(\n",
    "            pixel_values=pixel_values,\n",
    "        )\n",
    "\n",
    "        return last_hidden_state, self.visual_projection(pooled_output)\n",
    "\n",
    "    def clip_loss(self, similarity: torch.Tensor) -> torch.Tensor:\n",
    "        caption_loss = nn.functional.cross_entropy(\n",
    "            similarity, torch.arange(len(similarity))\n",
    "        )\n",
    "        image_loss = nn.functional.cross_entropy(\n",
    "            similarity.t(), torch.arange(len(similarity.t()))\n",
    "        )\n",
    "        return (caption_loss + image_loss) / 2.0\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        pixel_values: torch.FloatTensor,\n",
    "    ) -> Tuple[\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "    ]:\n",
    "        image_last_hidden_states, image_embeds = self.get_image_features(\n",
    "            pixel_values=pixel_values,\n",
    "        )\n",
    "\n",
    "        text_last_hidden_states, text_embeds = self.get_text_features(\n",
    "            input_ids=input_ids,\n",
    "        )\n",
    "\n",
    "        image_embeds = image_embeds / image_embeds.norm(dim=1, keepdim=True)\n",
    "        text_embeds = text_embeds / text_embeds.norm(dim=1, keepdim=True)\n",
    "\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits_per_image = logit_scale * image_embeds @ text_embeds.t()\n",
    "        logits_per_text = logits_per_image.t()\n",
    "\n",
    "        return (\n",
    "            logits_per_image,\n",
    "            logits_per_text,\n",
    "            text_embeds,\n",
    "            image_embeds,\n",
    "            image_last_hidden_states,\n",
    "            text_last_hidden_states,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8644e3df",
   "metadata": {},
   "source": [
    "## prepare model file\n",
    "- download https://huggingface.co/openai/clip-vit-base-patch32/blob/main/pytorch_model.bin to /mnt/clip/clip-vit-base-patch32/pytorch_model.bin\n",
    "- download https://huggingface.co/openai/clip-vit-base-patch32/blob/main/merges.txt to /mnt/clip/clip-vit-base-patch32/merges.txt\n",
    "- download https://huggingface.co/openai/clip-vit-base-patch32/blob/main/vocab.json to /mnt/clip/clip-vit-base-patch32/vocab.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b4e0806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIPModel(\n",
      "  (text_model): CLIPTextModel(\n",
      "    (embeddings): CLIPTextEmbeddings(\n",
      "      (token_embedding): Embedding(49408, 512)\n",
      "      (position_embedding): Embedding(77, 512)\n",
      "    )\n",
      "    (encoder): CLIPEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (vision_model): CLIPVisionModel(\n",
      "    (embeddings): CLIPVisionEmbeddings(\n",
      "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
      "      (position_embedding): Embedding(50, 768)\n",
      "    )\n",
      "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (encoder): CLIPEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
      "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "torch.set_default_device(\"cuda\")\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "checkpoint_path = '/mnt/clip/clip-vit-base-patch32/pytorch_model.bin'\n",
    "state_dict = torch.load(checkpoint_path, map_location=\"cuda:0\")\n",
    "\n",
    "model_config = CLIPConfig()\n",
    "clip_model = CLIPModel(model_config)\n",
    "clip_model.load_state_dict(state_dict, strict=False)\n",
    "print(clip_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ab9e9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from functools import lru_cache\n",
    "from typing import *\n",
    "\n",
    "import regex as re\n",
    "\n",
    "import ftfy\n",
    "\n",
    "\n",
    "@lru_cache()\n",
    "def bytes_to_unicode():\n",
    "    \"\"\"\n",
    "    Returns list of utf-8 byte and a mapping to unicode strings. We specifically avoids mapping to whitespace/control\n",
    "    characters the bpe code barfs on.\n",
    "\n",
    "    The reversible bpe codes work on unicode strings. This means you need a large # of unicode characters in your vocab\n",
    "    if you want to avoid UNKs. When you're at something like a 10B token dataset you end up needing around 5K for\n",
    "    decent coverage. This is a significant percentage of your normal, say, 32K bpe vocab. To avoid that, we want lookup\n",
    "    tables between utf-8 bytes and unicode strings.\n",
    "    \"\"\"\n",
    "    bs = (\n",
    "        list(range(ord(\"!\"), ord(\"~\") + 1))\n",
    "        + list(range(ord(\"¡\"), ord(\"¬\") + 1))\n",
    "        + list(range(ord(\"®\"), ord(\"ÿ\") + 1))\n",
    "    )\n",
    "    cs = bs[:]\n",
    "    n = 0\n",
    "    for b in range(2**8):\n",
    "        if b not in bs:\n",
    "            bs.append(b)\n",
    "            cs.append(2**8 + n)\n",
    "            n += 1\n",
    "    cs = [chr(n) for n in cs]\n",
    "    return dict(zip(bs, cs))\n",
    "\n",
    "\n",
    "def get_pairs(word):\n",
    "    \"\"\"\n",
    "    Return set of symbol pairs in a word.\n",
    "\n",
    "    Word is represented as tuple of symbols (symbols being variable-length strings).\n",
    "    \"\"\"\n",
    "    pairs = set()\n",
    "    prev_char = word[0]\n",
    "    for char in word[1:]:\n",
    "        pairs.add((prev_char, char))\n",
    "        prev_char = char\n",
    "    return pairs\n",
    "\n",
    "\n",
    "class CLIPTokenizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_file,\n",
    "        merges_file,\n",
    "    ):\n",
    "        with open(vocab_file, encoding=\"utf-8\") as vocab_handle:\n",
    "            self.encoder = json.load(vocab_handle)\n",
    "\n",
    "        self.decoder = {v: k for k, v in self.encoder.items()}\n",
    "\n",
    "        self.byte_encoder = bytes_to_unicode()\n",
    "        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n",
    "\n",
    "        with open(merges_file, encoding=\"utf-8\") as merges_handle:\n",
    "            bpe_merges = (\n",
    "                merges_handle.read().strip().split(\"\\n\")[1 : 49152 - 256 - 2 + 1]\n",
    "            )  # =[1:]\n",
    "        bpe_merges = [tuple(merge.split()) for merge in bpe_merges]\n",
    "        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n",
    "\n",
    "        self.cache = {\n",
    "            \"<|startoftext|>\": \"<|startoftext|>\",\n",
    "            \"<|endoftext|>\": \"<|endoftext|>\",\n",
    "        }\n",
    "\n",
    "        self.pat = re.compile(\n",
    "            r\"\"\"<\\|startoftext\\|>|<\\|endoftext\\|>|'s|'t|'re|'ve|'m|'ll|'d|[\\p{L}]+|[\\p{N}]|[^\\s\\p{L}\\p{N}]+\"\"\",\n",
    "            re.IGNORECASE,\n",
    "        )\n",
    "\n",
    "        self.unk_token = \"<|endoftext|>\"\n",
    "        self.bos_token = \"<|startoftext|>\"\n",
    "        self.eos_token = \"<|endoftext|>\"\n",
    "        self.pad_token = \"<|endoftext|>\"\n",
    "\n",
    "        self.unk_token_id = 49407\n",
    "        self.bos_token_id = 49406\n",
    "        self.eos_token_id = 49407\n",
    "        self.pad_token_id = 49407\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.encoder)\n",
    "\n",
    "    def bpe(self, token):\n",
    "        if token in self.cache:\n",
    "            return self.cache[token]\n",
    "\n",
    "        word = tuple(token[:-1]) + (token[-1] + \"</w>\",)\n",
    "        pairs = get_pairs(word)\n",
    "\n",
    "        if not pairs:\n",
    "            return token + \"</w>\"\n",
    "\n",
    "        while True:\n",
    "            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float(\"inf\")))\n",
    "            if bigram not in self.bpe_ranks:\n",
    "                break\n",
    "            first, second = bigram\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                try:\n",
    "                    j = word.index(first, i)\n",
    "                except ValueError:\n",
    "                    new_word.extend(word[i:])\n",
    "                    break\n",
    "                else:\n",
    "                    new_word.extend(word[i:j])\n",
    "                    i = j\n",
    "\n",
    "                if word[i] == first and i < len(word) - 1 and word[i + 1] == second:\n",
    "                    new_word.append(first + second)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            new_word = tuple(new_word)\n",
    "            word = new_word\n",
    "            if len(word) == 1:\n",
    "                break\n",
    "            else:\n",
    "                pairs = get_pairs(word)\n",
    "        word = \" \".join(word)\n",
    "        self.cache[token] = word\n",
    "        return word\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = self._tokenize(text)\n",
    "        return [self.encoder.get(t, self.unk_token_id) for t in tokens]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        tokens = [self.decoder.get(index) for index in ids]\n",
    "        return self.convert_tokens_to_string(tokens)\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        bpe_tokens = []\n",
    "\n",
    "        text = ftfy.fix_text(text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "        text = text.strip().lower()\n",
    "\n",
    "        for token in re.findall(self.pat, text):\n",
    "            token = \"\".join(\n",
    "                self.byte_encoder[b] for b in token.encode(\"utf-8\")\n",
    "            )  # Maps all our bytes to unicode strings, avoiding control tokens of the BPE (spaces in our case)\n",
    "            bpe_tokens.extend(bpe_token for bpe_token in self.bpe(token).split(\" \"))\n",
    "        return bpe_tokens\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        text = \"\".join(tokens)\n",
    "        byte_array = bytearray([self.byte_decoder[c] for c in text])\n",
    "        text = byte_array.decode(\"utf-8\", errors=\"replace\").replace(\"</w>\", \" \").strip()\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bdbddec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[320, 1125, 539, 320, 2368]\n",
      "torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "vocab_file = '/mnt/clip/clip-vit-base-patch32/vocab.json'\n",
    "merges_file = '/mnt/clip/clip-vit-base-patch32/merges.txt'\n",
    "tokenizer = CLIPTokenizer(vocab_file, merges_file)\n",
    "\n",
    "input_ids = tokenizer.encode('a photo of a cat')\n",
    "\n",
    "print(input_ids)\n",
    "\n",
    "input_ids = torch.LongTensor([input_ids]).cuda()\n",
    "\n",
    "_, text_embeddings = clip_model.get_text_features(input_ids)\n",
    "text_embeddings = text_embeddings / text_embeddings.norm(dim=1,keepdim=True)\n",
    "\n",
    "text_embeddings = text_embeddings[0]\n",
    "print(text_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1c5fa93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 512])\n",
      "torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "image_mean = [0.48145466,0.4578275,0.40821073]\n",
    "image_std = [0.26862954,0.26130258,0.27577711]\n",
    "\n",
    "image_paths = ['./data/n02687172_aircraft_carrier.jpeg', './data/n02974003_car_wheel.jpeg']\n",
    "images = []\n",
    "for image_path in image_paths:\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    resized_image = image.resize((224,224), resample=Image.Resampling.BICUBIC)\n",
    "    image_arr = np.array(resized_image) # (height,width,3)\n",
    "    image_arr = image_arr * 1/225.0\n",
    "    normalized_image_arr = (image_arr - image_mean) / image_std\n",
    "    normalized_image_arr = normalized_image_arr.transpose(2, 0, 1) # (c, h, w)\n",
    "    images.append(normalized_image_arr)\n",
    "\n",
    "images = np.array(images)\n",
    "    \n",
    "    \n",
    "# inference\n",
    "pixel_values = torch.Tensor(images).cuda()\n",
    "_, image_embeddings = clip_model.get_image_features(pixel_values)    \n",
    "image_embeddings = image_embeddings / image_embeddings.norm(dim=1,keepdim=True)\n",
    "print(image_embeddings.shape)\n",
    "\n",
    "# mean\n",
    "image_embeddings_mean = torch.mean(image_embeddings,dim=0)\n",
    "\n",
    "print(image_embeddings_mean.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd2932a1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7fdcfdaad270>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "sns.displot(np.array(image_embeddings_mean.tolist()))\n",
    "sns.displot(np.array(text_embeddings.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9535619",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
